{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# WebRTC AI Voice Chat with LangChain\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/colinmcnamara/austin_langchain/blob/main/labs/LangChain_107/webrtc-ai-voice-chat/webrtc-ai-voice-chat.ipynb)\n",
        "\n",
        "\n",
        "This lab will demonstrate how you can use technologies like LangChain, WebRTC, Speech to Text, and Text to Audio, to build and talk with your AI applications.\n",
        "\n",
        "This lab relies on the following technologies:\n",
        "* [LangChain](https://www.langchain.com/) - to develop AI applications\n",
        "* [OpenAI Whisper](https://openai.com/index/whisper/) - to transcribe your speech\n",
        "* [Suno Bark](https://github.com/suno-ai/bark) - to synthesize audio from text\n",
        "* [WebRTC](https://webrtc.org/) - to allow communication between your browser application and the server running the AI application.\n",
        "\n",
        "WebRTC is a peer to peer technology that allows you to transmit and receive audio, video, and other type of data between devices.\n",
        "\n",
        "The LangChain AI application, OpenAI Whisper, and Suno Bark models all run on the server.\n",
        "\n",
        "This lab requires `python` version `3.12`, however the version colab provides is `3.10`. In order to run this lab, you will need to upgrade our `python` version.\n",
        "\n",
        "## Upgrading Python"
      ],
      "metadata": {
        "id": "WOdzqy7bM53b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlhf0A6i0kSR",
        "outputId": "d058b909-6162-4b8e-b498-1bc5974fff2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%%bash\n",
        "apt-get install python3.12 python3-pip python3.12-distutils python3.12-dev python3.12-venv\n",
        "sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 1\n",
        "sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.12 2"
      ],
      "metadata": {
        "id": "LIcR5NTS0rCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZxgh2SBIeMn",
        "outputId": "998b76b7-3bcf-4a65-f17c-37d002ae0036"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.12.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%%bash\n",
        "python -m ensurepip --upgrade\n",
        "python -m pip install --upgrade setuptools\n"
      ],
      "metadata": {
        "id": "Pxm2bLO17fpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Ollama"
      ],
      "metadata": {
        "id": "fs7ant2wN7i9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "id": "bt7koIPJN4ru",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "886b1411-3f22-4338-ec72-0335de40cd7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ">>> Downloading ollama...\n",
            "#=#=- #                                                                                            \r##O=#  #                                                                                           \r\r############################################################################################# 100.0%#=#=-#   #                                                                                         \r #=#=-#    #                                                                                       \r #=O#-  #    #                                                                                     \r\r                                                                                                0.0%\r                                                                                                0.3%\r                                                                                                0.9%\r#                                                                                               1.7%\r##                                                                                              3.0%\r####                                                                                            5.1%\r#######                                                                                         8.3%\r############                                                                                   13.5%\r###################                                                                            21.0%\r##########################                                                                     28.3%\r###############################                                                                33.9%\r###################################                                                            38.2%\r########################################                                                       43.1%\r#############################################                                                  48.6%\r#################################################                                              53.5%\r#######################################################                                        59.2%\r############################################################                                   65.4%\r#################################################################                              70.5%\r#######################################################################                        76.7%\r#############################################################################                  83.5%\r######################################################################################         93.3%\r############################################################################################# 100.0%\n",
            ">>> Installing ollama to /usr/local/bin...\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash --bg\n",
        "ollama serve &> ollama.log 2>&1\n"
      ],
      "metadata": {
        "id": "i0-J2KujyV9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ollama takes a minute to start and get running so we wait..."
      ],
      "metadata": {
        "id": "0zK6rRj6TZvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!until ollama list; do echo waiting for ollama; sleep 5; done"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUlANeuDJz_M",
        "outputId": "23db8aa7-29c0-4bcd-c262-2cf79fa540f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME\tID\tSIZE\tMODIFIED \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now download Phi3 small language model."
      ],
      "metadata": {
        "id": "xwoRzfqeTnNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash --bg\n",
        "ollama pull phi3 >> phi3.log 2>&1"
      ],
      "metadata": {
        "id": "j_Lg6d7fyfnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " This step also takes some time, so we wait some more..."
      ],
      "metadata": {
        "id": "xmcn7QDbT0KC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!until ollama list | grep phi3; do echo waiting for phi; sleep 5; done"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kds_RcRv1oT4",
        "outputId": "e8ec4d90-e1b7-4833-b3d1-7b4eb112b494"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "waiting for phi\n",
            "waiting for phi\n",
            "waiting for phi\n",
            "waiting for phi\n",
            "waiting for phi\n",
            "waiting for phi\n",
            "phi3:latest\ta2c89ceaed85\t2.3 GB\t3 seconds ago\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next step, download the source code for the WebRTC AI Voice Chat demo and prepare our workspace"
      ],
      "metadata": {
        "id": "FOdtsg8kT1wK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "git clone https://github.com/lalanikarim/webrtc-ai-voice-chat.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpY-HhBMyy4S",
        "outputId": "73576e5a-a308-4a68-93ba-ed94d3ee44b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cloning into 'webrtc-ai-voice-chat'...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd webrtc-ai-voice-chat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOFT-xDczND-",
        "outputId": "085670f2-63ab-42a2-8636-e8dfd3121d4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/webrtc-ai-voice-chat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare our Python workspace and install project's dependencies. This will take some time. You can check pip.log file to monitor the progress...."
      ],
      "metadata": {
        "id": "Fi1-2LmDT9cB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "python -m venv .venv\n",
        "source .venv/bin/activate\n",
        "pip install -r requirements.txt > ../pip.log"
      ],
      "metadata": {
        "id": "ZBgOpf9j2Vm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now check to make sure Ollama is still running"
      ],
      "metadata": {
        "id": "RoQEubkgUCzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "ollama list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QoSEc6zA0iP",
        "outputId": "c06e05e8-2743-4562-c575-dbce7bd3bd5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME       \tID          \tSIZE  \tMODIFIED      \n",
            "phi3:latest\ta2c89ceaed85\t2.3 GB\t8 minutes ago\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we start the Python server. This will also start the download of the Whisper and Bark models."
      ],
      "metadata": {
        "id": "o4tesTuBUIFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash --bg\n",
        "source .venv/bin/activate\n",
        "python -m server --port 8888 > ../webrtc.log 2>&1"
      ],
      "metadata": {
        "id": "gnfPCdfG8_Rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the server takes a few seconds to star up, let's wait some more..."
      ],
      "metadata": {
        "id": "M_nrxI8aUUyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!until curl -I http://localhost:8888; do echo waiting for server; sleep 5; done"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZMwLyqOLa_T",
        "outputId": "7fae1944-10a3-4b0d-a6d0-bee41f8e74a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "curl: (7) Failed to connect to localhost port 8888 after 0 ms: Connection refused\n",
            "waiting for server\n",
            "curl: (7) Failed to connect to localhost port 8888 after 0 ms: Connection refused\n",
            "waiting for server\n",
            "curl: (7) Failed to connect to localhost port 8888 after 0 ms: Connection refused\n",
            "waiting for server\n",
            "curl: (7) Failed to connect to localhost port 8888 after 0 ms: Connection refused\n",
            "waiting for server\n",
            "curl: (7) Failed to connect to localhost port 8888 after 0 ms: Connection refused\n",
            "waiting for server\n",
            "curl: (7) Failed to connect to localhost port 8888 after 0 ms: Connection refused\n",
            "waiting for server\n",
            "curl: (7) Failed to connect to localhost port 8888 after 0 ms: Connection refused\n",
            "waiting for server\n",
            "HTTP/1.1 200 OK\n",
            "\u001b[1mContent-Type\u001b[0m: text/html; charset=utf-8\n",
            "\u001b[1mContent-Length\u001b[0m: 4208\n",
            "\u001b[1mDate\u001b[0m: Wed, 08 May 2024 22:41:09 GMT\n",
            "\u001b[1mServer\u001b[0m: Python/3.12 aiohttp/3.9.5\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the public IP for your Colab instance. We will need this IP to gain access to our app in the browser later."
      ],
      "metadata": {
        "id": "NYVwbHR2UeNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "curl ipv4.icanhazip.com 2>/dev/null"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kptc1Pz_-efP",
        "outputId": "4cf353a1-8700-4bf8-deeb-aed1770b185c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.171.216.23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start the local tunnel"
      ],
      "metadata": {
        "id": "DyP3tMnBUrWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash --bg\n",
        "npx localtunnel --port 8888 > ../localtunnel.log"
      ],
      "metadata": {
        "id": "bEQDkoRP_Nqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grab the url generated by the local tunnel"
      ],
      "metadata": {
        "id": "BxRJchVnU6J-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat ../localtunnel.log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8CcHBgUQ2Ag",
        "outputId": "cfa89292-e825-4a62-daac-647c46e488e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "your url is: https://tiny-buttons-carry.loca.lt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Chrome works best for this demo. Firefox may not recover from WebRTC failures reliably.\n",
        "\n",
        "Note: At anytime, if Ollama or the Python server stop running, you'll need to restart them by running the appropriate cells. This might happen for a couple of times before the server runs with more stability."
      ],
      "metadata": {
        "id": "M0p3zapOU7xX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mD9Q5rViVWad"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}